{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MVP — Online Retail II (versão funcional)\n",
        "\n",
        "Notebook pronto para executar no Google Colab. Organização similar ao checklist solicitado: carga, limpeza, EDA, engenharia de atributos (RFM), segmentação (KMeans), tarefa supervisonada (recompra 90 dias), baselines, otimização e avaliação.\n",
        "\n",
        "Se a importação automática falhar, siga as instruções da célula de carregamento para fazer upload manual do arquivo Excel/CSV do dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalações (Colab): descomente se necessário\n",
        "!pip install --quiet ucimlrepo xgboost\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "print('Ambiente pronto. Random seed fixada:', RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregamento do dataset: tentar via ucimlrepo; se falhar, peça upload manual\n",
        "df = None\n",
        "try:\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "    print('Tentando baixar via ucimlrepo (UCI)...')\n",
        "    ds = fetch_ucirepo(id=502)  # id conhecido do Online Retail II\n",
        "    # tentar extrair DataFrame\n",
        "    if hasattr(ds.data, 'features'):\n",
        "        df = ds.data.features.copy()\n",
        "    else:\n",
        "        try:\n",
        "            df = pd.DataFrame(ds.data)\n",
        "        except Exception:\n",
        "            df = None\n",
        "except Exception as e:\n",
        "    print('ucimlrepo não funcionou automaticamente:', str(e))\n",
        "\n",
        "if df is None:\n",
        "    print('\\nNão foi possível carregar automaticamente o dataset. Por favor faça upload manual do arquivo:')\n",
        "    print('1) Baixe o arquivo Online Retail II (ex: Excel) do repositório UCI no seu navegador.')\n",
        "    print('2) No Colab: clique em Arquivo → Upload e selecione o arquivo, ou use o código abaixo para fazer upload interativo.')\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    # tenta identificar o arquivo carregado\n",
        "    for name in uploaded:\n",
        "        try:\n",
        "            if name.lower().endswith('.xlsx'):\n",
        "                df = pd.read_excel(name)\n",
        "            else:\n",
        "                df = pd.read_csv(name)\n",
        "            print('Arquivo carregado e lido:', name)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print('Falha ao ler', name, str(e))\n",
        "\n",
        "if df is None:\n",
        "    raise RuntimeError('Nenhum dataset carregado. Interrompendo. Por favor carregue o arquivo Online Retail II no Colab e execute novamente.')\n",
        "\n",
        "print('Dimensão inicial do dataset:', df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limpeza e preparação inicial\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "if 'InvoiceDate' in df.columns:\n",
        "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
        "if 'Quantity' in df.columns and 'UnitPrice' in df.columns:\n",
        "    df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "print('Valores nulos por coluna:')\n",
        "print(df.isnull().sum())\n",
        "\n",
        "if 'CustomerID' in df.columns:\n",
        "    df = df.dropna(subset=['CustomerID'])\n",
        "    df['CustomerID'] = df['CustomerID'].astype(int)\n",
        "\n",
        "if 'InvoiceNo' in df.columns:\n",
        "    df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "if 'Quantity' in df.columns:\n",
        "    df = df[df['Quantity']>0]\n",
        "\n",
        "print('Dimensão após limpeza:', df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Engenharia RFM\n",
        "snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
        "    'InvoiceNo': 'nunique',\n",
        "    'TotalPrice': 'sum'\n",
        "}).reset_index()\n",
        "rfm.columns = ['CustomerID','Recency','Frequency','Monetary']\n",
        "rfm.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segmentação com KMeans (usar log1p + StandardScaler)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X_rfm = rfm[['Recency','Frequency','Monetary']].copy()\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(np.log1p(X_rfm))\n",
        "\n",
        "scores = {}\n",
        "for k in range(2,7):\n",
        "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE)\n",
        "    labels = km.fit_predict(X_scaled)\n",
        "    scores[k] = silhouette_score(X_scaled, labels)\n",
        "\n",
        "print('Silhouette scores por k:', scores)\n",
        "best_k = max(scores, key=scores.get)\n",
        "print('Melhor k:', best_k)\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_STATE)\n",
        "rfm['Cluster'] = kmeans.fit_predict(X_scaled)\n",
        "rfm.groupby('Cluster').agg({'Recency':'median','Frequency':'median','Monetary':'median','CustomerID':'count'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definição do target supervisonado: recompra em 90 dias\n",
        "max_date = df['InvoiceDate'].max()\n",
        "cutoff_date = max_date - pd.Timedelta(days=180)\n",
        "label_window = pd.Timedelta(days=90)\n",
        "\n",
        "df_features = df[df['InvoiceDate'] <= cutoff_date].copy()\n",
        "df_labels = df[(df['InvoiceDate'] > cutoff_date) & (df['InvoiceDate'] <= cutoff_date + label_window)].copy()\n",
        "feature_customers = set(df_features['CustomerID'].unique())\n",
        "label_customers = set(df_labels['CustomerID'].unique())\n",
        "\n",
        "snapshot_ref = cutoff_date + pd.Timedelta(days=1)\n",
        "rfm_feat = df_features.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (snapshot_ref - x.max()).days,\n",
        "    'InvoiceNo': 'nunique',\n",
        "    'TotalPrice': 'sum'\n",
        "}).reset_index()\n",
        "rfm_feat.columns = ['CustomerID','Recency','Frequency','Monetary']\n",
        "rfm_feat['target_repurchase_90d'] = rfm_feat['CustomerID'].apply(lambda x: 1 if x in label_customers else 0)\n",
        "\n",
        "print('Tamanho features:', rfm_feat.shape)\n",
        "print(rfm_feat['target_repurchase_90d'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split treino/teste e escala\n",
        "X = rfm_feat[['Recency','Frequency','Monetary']]\n",
        "y = rfm_feat['target_repurchase_90d']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
        "scaler2 = StandardScaler()\n",
        "X_train_scaled = scaler2.fit_transform(np.log1p(X_train))\n",
        "X_test_scaled = scaler2.transform(np.log1p(X_test))\n",
        "print('Train:', X_train.shape, 'Test:', X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baselines: Logistic Regression e RandomForest\n",
        "lr = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
        "t0 = time.time(); lr.fit(X_train_scaled, y_train); t1 = time.time()\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "y_prob_lr = lr.predict_proba(X_test_scaled)[:,1]\n",
        "print('LR time (s):', t1-t0)\n",
        "print('LR - Accuracy:', accuracy_score(y_test,y_pred_lr), 'ROC AUC:', roc_auc_score(y_test,y_prob_lr))\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "t0 = time.time(); rf.fit(X_train, y_train); t1 = time.time()\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_prob_rf = rf.predict_proba(X_test)[:,1]\n",
        "print('RF time (s):', t1-t0)\n",
        "print('RF - Accuracy:', accuracy_score(y_test,y_pred_rf), 'ROC AUC:', roc_auc_score(y_test,y_prob_rf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplos de RandomizedSearch (curto) para RandomForest\n",
        "param_dist = {\n",
        "    'n_estimators': [50,100,200],\n",
        "    'max_depth': [None, 5, 10, 20],\n",
        "    'min_samples_split': [2,5,10]\n",
        "}\n",
        "rs = RandomizedSearchCV(RandomForestClassifier(random_state=RANDOM_STATE), param_distributions=param_dist, n_iter=8, cv=3, scoring='roc_auc', random_state=RANDOM_STATE, n_jobs=-1)\n",
        "t0 = time.time(); rs.fit(X_train, y_train); t1 = time.time()\n",
        "print('RandomizedSearch time (s):', t1-t0)\n",
        "print('Best params:', rs.best_params_)\n",
        "best_rf = rs.best_estimator_\n",
        "y_pred_best = best_rf.predict(X_test)\n",
        "y_prob_best = best_rf.predict_proba(X_test)[:,1]\n",
        "print('Best RF - Accuracy:', accuracy_score(y_test,y_pred_best), 'ROC AUC:', roc_auc_score(y_test,y_prob_best))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "print(classification_report(y_test, y_pred_best))\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title('Confusion Matrix - Best RF')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observações finais\n",
        "- Fixe random_state para reprodutibilidade.\n",
        "- Se quiser que eu adapte o notebook para outro dataset ou gere um notebook com **deploy** (por exemplo, exportar modelo para o GCP/AWS ou salvar artefatos no Drive), eu faço.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}